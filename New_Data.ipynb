{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac95740-f1e4-4942-94c1-2a5768f8c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24700ca2-e9f0-42bd-9af6-e836861c3461",
   "metadata": {},
   "source": [
    "1. Check for null data... step 1: check for it, throw an error if null data occurs in a non-null column (Non-nulls are log, date, WO name, ex id, set index,  set, measurement) HAVE TO DO THIS AFTER PROCESSING AS WELL, before merge? </ for the first check\n",
    "2. Drop notes column </\n",
    "3. Convert Date data </\n",
    "4. Convert exercise name to exercise_id </\n",
    "5. Convert null data to 0's???\n",
    "6. log_ID should be incremented?\n",
    "7. set_index must be added (Set index hasn't been used for anything... can probably just drop this...)\n",
    "8. Account for new exercises that may not be present (will need exercise converting to exercise_id, mapping to other tables...)\n",
    "9. Make sure columns are in the correct order!\n",
    "10. Merge the new df with the original\n",
    "11. Still have to delete the pre-March data (incorporated to #3)\n",
    "12. Handling new exercises (ideally, I'd like it to throw an error if an exercise isn't found, offer to add the exercise, and run the script for adding new exercises, which I have not yet created)\n",
    "\n",
    "Errors:\n",
    "If a new exercise is detected, it should be easy enough to map it into the exercises table and create the ID etc. for it. It will not have the muscle data it needs.\n",
    "Solution: Throw an error if this is the case, write a new function that will add an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb3d692-c122-4d26-8c4b-cd453423f9b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv)\n\u001b[0;32m      3\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m new_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNotes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "def add_new_data(csv): #Enter file name here as 'file_name.csv'\n",
    "    new_data = pd.read_csv(csv)\n",
    "    nd = new_data.copy()\n",
    "    nd = nd.drop(columns=['Notes'])\n",
    "\n",
    "    nd['Date'] = pd.to_datetime(df['Date']).dt.date #Should adjust the date format\n",
    "\n",
    "    nd['set_index'] = (df['Set'] == df['set_index'].max() + 1).cumsum() #Used initially to for the set_index. Probs won't work at this stage, because it'll start from 1. \\\n",
    "    #Could use a max formula to get the current highest and start from there? Lambda formula?\n",
    "\n",
    "    exercise_map = ex.set_index('exercise_name')['exercise_id'].to_dict() #.set_index sets the specified column as an index (ex_name), and the ID as the value\n",
    "    nd['exercise_id'] = nd['Exercise'].map(exercise_map) #Used to \n",
    "    nd = nd.drop(columns=['Exercise'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29619529-1159-4b7e-a74c-0efcefbb5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Check for null data in non-null columns\n",
    "\n",
    "required_fields = ['Date','Workout name','Exercise','Set','Measurement unit']\n",
    "\n",
    "def validate_non_null_fields_start(nd, required_fields):\n",
    "    try:\n",
    "        null_check = df[required_fields].isnull().any()\n",
    "\n",
    "        # If any required field has nulls, raise an error\n",
    "        if null_check.any(): #.any returns True if any value is True in the check\n",
    "            null_columns = null_check[null_check].index.tolist()\n",
    "            raise ValueError(f\"Null values found in required fields: {null_columns}\")\n",
    "        \n",
    "        print(\"✅ No nulls found in required fields.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Validation Error: {e}\")\n",
    "        return nd[nd[required_fields].isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb9b180-f58f-4148-8a75-deb830bacede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Drop Notes column\n",
    "\n",
    "def drop_notes():\n",
    "    nd = nd.drop(columns=['Notes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139cb9b-bc0c-418e-975b-9bc19a48ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Adjust the date column datatype\n",
    "\n",
    "def adjust_date():\n",
    "    nd['Date'] = pd.to_datetime(nd['Date'], format = '%d/%m/%Y %H:%M').dt.date #Should convert the date and extract only date values\n",
    "\n",
    "#NOTE: The below is only for my current data as the pre-March data will mess with analysis\n",
    "def cutoff_date():\n",
    "    cutoff_date = pd.Timestamp('2025-03-01')\n",
    "    nd = nd[nd['Date'] >= cutoff_date]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a64d48-5b0d-4903-bf2a-19858b4bb854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convert Exercise Name to Exercise ID\n",
    "def ex_to_ID():\n",
    "    try:\n",
    "        exercise_map = ex.set_index('exercise_name')['exercise_id'].to_dict() #.set_index sets the specified column as an index (ex_name), and the ID as the value\n",
    "        nd['exercise_id'] = nd['Exercise'].map(exercise_map)\n",
    "        nd = nd.drop(columns=['Exercise'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b9bf2-5347-4498-80bc-606410b6ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Convert any null weights\n",
    "def null_values_zero:\n",
    "    nd['Weight'] = nd['Weight'].fillna(0)\n",
    "    nd['Distance'] = nd['Distance'].fillna(0)\n",
    "    nd['Duration'] = nd['Duration'].fillna(0)\n",
    "    nd['Reps'] = nd['Reps'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec05308-79f7-48aa-835a-31a57c4d283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Log ID - I don't think this is necessary here, so just reset the index when merging?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319b7eb-af18-4fc9-ab25-52b027369381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Set index - Can be done before merge\n",
    "\n",
    "def new_set_index:\n",
    "    last_set_index = df['set_index'].max()\n",
    "    nd['set_index'] = (nd['Set'] == 1).cumsum() + last_set_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b33ae1-dcfd-40b1-8b65-8b938b4ca18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61fe08-ca9e-433b-8927-607b4a64f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Column Order\n",
    "\n",
    "def set_column_order:\n",
    "    columns_order = ['log_ID', 'Date', 'Workout name', 'exercise_id', 'set_index', 'Set', 'Weight', 'Reps', 'Distance', 'Duration', 'Measurement unit']\n",
    "    nd = nd[columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7b228-447e-4232-a69e-7fad08df6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Merge the datasets\n",
    "df_combined = pd.concat([df, nd]) #ignore index bit?\n",
    "# Above is one option... but duplicates?\n",
    "\n",
    "#Handling dupes\n",
    "df_final = df_combined.drop_duplicates(subset='XXXXXXXXX log ID goes here', keep='last')\n",
    "\n",
    "# Combine both\n",
    "df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
    "\n",
    "# Drop duplicates by the unique key, keeping the updated version (assumes it's last)\n",
    "df_final = df_combined.drop_duplicates(subset='unique_key_column', keep='first')\n",
    "\n",
    "# Save or use\n",
    "df_final.to_csv(\"merged_data.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
